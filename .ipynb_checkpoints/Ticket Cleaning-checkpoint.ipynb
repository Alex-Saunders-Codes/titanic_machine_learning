{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18c0502e-01cc-4a01-b780-7ac3cc750cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d2f8daa-b93c-407b-a290-4a68a99f90cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefix_check(string):\n",
    "    if string.isdigit():\n",
    "        return 'None'\n",
    "    else:\n",
    "        string = string.replace('.', '').replace('/', '')\n",
    "        return string\n",
    "\n",
    "def mean_encode_map(series, target):\n",
    "    test_df = pd.concat([series, target], axis=1)\n",
    "    incidence = series.value_counts().to_dict()\n",
    "    mean_map = test_df.groupby(0).mean().to_dict()['Survived']\n",
    "    average_value = sum(mean_map.values())/len(mean_map)\n",
    "    for value in mean_map:\n",
    "        if incidence[value] < 3:\n",
    "            mean_map[value] = average_value\n",
    "    return mean_map\n",
    "    \n",
    "def ticket_prefix(train, test):    \n",
    "    tick = train['Ticket']\n",
    "    space_split = tick.str.split(expand=True)\n",
    "    tick_prefix = space_split[0].apply(prefix_check)\n",
    "    mean_map = mean_encode_map(tick_prefix, train['Survived'])\n",
    "    train['ticket_prefix'] = tick_prefix.map(mean_map)\n",
    "    test_prefix = test['Ticket'].str.split(expand=True)[0].apply(prefix_check)\n",
    "    test['ticket_prefix'] = test_prefix.map(mean_map)\n",
    "    test['ticket_prefix'].fillna(test['ticket_prefix'].mean(), inplace=True)\n",
    "    return train, test\n",
    "\n",
    "def cleanTitanic(path1, path2):\n",
    "    train = pd.read_csv(path1)\n",
    "    test = pd.read_csv(path2)\n",
    "    group_update = [train, test]\n",
    "    for item in group_update:\n",
    "        values = {'Age': item['Age'].mean(), 'Embarked':item['Embarked'].mode()[0] }\n",
    "        item.fillna(value = values, inplace=True)\n",
    "        item['female'] = pd.get_dummies(item['Sex'])['female']\n",
    "        emb = pd.get_dummies(item['Embarked'])\n",
    "        item['Emb Cherbourg'] = emb['C']\n",
    "        item['Emb Queenstown'] = emb['Q']\n",
    "        item['Emb Southampton'] = emb['S']\n",
    "    train, test = ticket_prefix(train, test)\n",
    "    return train, test\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35efd2c6-957d-407f-9289-c88ae5734ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, predict = cleanTitanic('titanic_data/train (1).csv', 'titanic_data/test_2.csv' )\n",
    "X = data[['Pclass',  'Age', 'SibSp',\n",
    "       'Parch', 'Fare', 'female', 'Emb Cherbourg',\n",
    "       'Emb Queenstown', 'Emb Southampton', 'ticket_prefix']]\n",
    "y = data['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc2f43a4-dadf-4120-ab95-9b479955385f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
       "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'female',\n",
       "       'Emb Cherbourg', 'Emb Queenstown', 'Emb Southampton', 'ticket_prefix'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c6a94528-221e-46a6-8c62-b8c99b98b7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b324de0-f274-4574-8fba-b6f86d5f127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_fit_and_predict(X_train, X_test, y_train, y_test, predict):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn import preprocessing\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_scaled = scaler.transform(X_train)\n",
    "    classifier = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    classifier.fit(X_scaled, y_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    print(f\"Training Data Score: {classifier.score(X_scaled, y_train)}\")\n",
    "    print(f\"Testing Data Score: {classifier.score(X_test, y_test)}\")\n",
    "    predict_sub = predict[['Pclass',  'Age', 'SibSp',\n",
    "       'Parch', 'Fare', 'female', 'Emb Cherbourg',\n",
    "       'Emb Queenstown', 'Emb Southampton', 'ticket_prefix']]\n",
    "    predict_scaled = scaler.transform(predict_sub)\n",
    "    survived = classifier.predict(predict_scaled)\n",
    "    predictions = pd.DataFrame({'PassengerId':list(predict['PassengerId']), 'Survived':list(survived)})\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "781f1a6f-d5dc-428e-a061-871062e56eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Score: 0.7846441947565543\n",
      "Testing Data Score: 0.651685393258427\n"
     ]
    }
   ],
   "source": [
    "predictions = logistic_fit_and_predict(X_train, X_test, y_train, y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "153192c4-0d56-4478-8e01-0cc1947bc8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_and_predict(X_train, X_test, y_train, y_test, predict):\n",
    "    from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "    from sklearn import preprocessing\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_scaled = scaler.transform(X_train)\n",
    "    classifier = KNN(leaf_size= 30, n_neighbors= 10, weights= 'uniform', random_state=42)\n",
    "    classifier.fit(X_scaled, y_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    print(f\"Training Data Score: {classifier.score(X_scaled, y_train)}\")\n",
    "    print(f\"Testing Data Score: {classifier.score(X_test, y_test)}\")\n",
    "    predict_sub = predict[['Pclass',  'Age', 'SibSp',\n",
    "       'Parch', 'Fare', 'female', 'Emb Cherbourg',\n",
    "       'Emb Queenstown', 'Emb Southampton', 'ticket_prefix']]\n",
    "    predict_scaled = scaler.transform(predict_sub)\n",
    "    survived = classifier.predict(predict_scaled)\n",
    "    predictions = pd.DataFrame({'PassengerId':list(predict['PassengerId']), 'Survived':list(survived)})\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9924527b-d058-42ce-bea8-a6181d59446c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Score: 0.7846441947565543\n",
      "Testing Data Score: 0.651685393258427\n"
     ]
    }
   ],
   "source": [
    "predictions = logistic_fit_and_predict(X_train, X_test, y_train, y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e430a3e-ac11-479d-80cd-dcbf63e1b4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv('submissions/s10_19_6(Ticket_Cleaned).csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dcd05f-1810-4e63-9f1c-4587d6acbbf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
